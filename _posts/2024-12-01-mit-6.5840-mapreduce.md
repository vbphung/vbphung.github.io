---
layout: post
title: Just completed MIT 6.5840 MapReduce Lab
date: 2024-12-01
---

There's been a huge update on this blog, but you can find the old version of it at [9dce7f5](https://github.com/vbphung/vbphung.github.io/tree/9dce7f5f67ab021e5108a139ce71f9893d980f6d).

Additionally, I won't copy-paste the MapReduce's concepts here so [MapReduce (2004)](https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf). In fact, I just learned MapReduce for fun because it's been already marked as deprecated in 2014 by its father, Google.

# Implementation

Actually, I found this lab not that challenging thanks to a bunch of useful hints provided at the bottom of the lab requirements. I just stuck to all of the hints, except one about `os.Rename`, which'll be detailed in the next section.

Overall, we have one Coordinator that distributes tasks for multiple Workers. Plus, all Map tasks must be completed before any Reduce task can start.

## Workers

Besides the Mapping/Reducing functions assigned on bootstrap, a Worker must generate its own ID to distinguish itself from others. And my Workers do this in the silliest way:

```go
workerID := rand.Int()
```

After starting, every Worker asks the Coordinator for a task periodically. If there's no task, the Worker will `time.Sleep(3 * time.Second)` before retrying. Else, the Worker will execute their assigned task then report the result back to the Coordinator.

### Map tasks handling

Each Map task asks Workers to process one single input file and write the result to the correct output files, which significantly matters to the final result.

1. Obtain the list of `KeyValue` by running `mapf` on the input file's content
2. Store each pair into the appropriate temporary `mr-${TaskID}-${i}(.txt)` file, where `i` is calculated by `ihash(kv.Key) % nReduce`.
3. Persist those temporary files to disk
4. Submit the result

### Reduce tasks handling

Tasks for Workers during the Reduce phase involve processing multiple intermediate files outputted from the Map phase.

It's notable that we have to aggregate all values of the same key for input of the `reducef` function. One effective way (not allocating extra space) is sorting the intermediate files by key.

```go
slices.SortFunc(kvs, func(a, b KeyValue) int {
    return strings.Compare(a.Key, b.Key)
})

for i := 0; i < len(kvs); {
    var vals []string
    for j := i; j < len(kvs) && kvs[j].Key == kvs[i].Key; j++ {
        vals = append(vals, kvs[j].Value)
    }
}
```

The results of all keys are finally concatenated and written to the `mr-out-${TaskID}(.txt)` output file.

## Coordinator

The tasks of the Coordinator are no more than assigning Map/Reduce tasks to Workers and monitoring the status of these tasks, which is straightforward to implement.

However, as written, the Coordinator can't reliably notice crashes from Workers. The solution is to launch a goroutine that periodically checks then marks timed-out tasks' status back as `Idle`.

```go
for _, s := range c.maps {
    if s != nil && s.Status == InProgress && t.Sub(s.AssignedAt).Seconds() > 5 {
        s.Status = Idle
        fmt.Printf("task map %d timeout\n", s.Task.ID)
    }
}
```

# Go's `os.Rename` and Linux's VFS
